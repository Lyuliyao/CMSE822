\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{mathabx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{biblatex} %Imports biblatex package
\addbibresource{reference.bib} %Import the bibliography file
\begin{document}

\section{Abstract}

\section{Introduction}
Poisson's equation is an elliptic partial differential equation of broad utility in theoretical physics. For example, the solution to Poisson's equation is the potential field caused by a given electric charge or mass density distribution; with the potential field known, one can then calculate electrostatic or gravitational (force) field. 

There are increasing number of method regarding solving this link of partial differential equation (PDE), such as finite difference method, finite element method and spectral method. 

In this project, we want to try MPI and OpenMP to solve Poisson equation
\begin{equation}
	\Delta u(\mathbf{x}) = f, \quad \mathbf{x} \in \Gamma
\end{equation}
in a irregular domain $\Gamma$ with Dirichlet boundary condition $u(\mathbf{x}) = g(\mathbf{x}), \mathbf{x}\in \partial\Gamma $ with finite element method.
  
\section{Results}
\subsection{One dimensional case}
\subsubsection{formula of finite element method}
Consider problem  
\begin{equation}
\label{equ:poisson_equation}
	u''(x) = f(x) \quad x\in (0,1)
\end{equation}
with boundary condition $u(x)=0$ at the boundary. We introduce a triangulation of the domain $\Omega = [0,1] $ into nonoverlapping elements:
\begin{equation}
\label{equ:triangulation}
	T_h = \{K_1,K_2,...\}
\end{equation}
such that $\Omega = \cup_{K\in T_h}K$. Now consider the space of continuous functions that are piecewise linear on the triangulation and zeros at the end points:
\begin{equation}
\label{equ:basis}
	V_h =\{v\in C^0([0,1]):v|_{K} \in \mathcal{P}_1(K) \forall K \in T_h, v(0) = v(1) = 0\}.
\end{equation}
Here $\mathcal{P}_p(K)$ is the space of polynomials on $K$ of degree at most $p$. Define a basis $\{\varphi_i\}$ for $V_h$ by the basis functions $\varphi_i\in V_h$ with $\varphi_i(x_j) = \delta_{ij}$, for $i,j = 1,...n$. The figure is shown in the following figure
\begin{figure}[htp]
\centering
	\includegraphics[width=0.5\textwidth]{basis_figure}
	\caption{what}
\end{figure}
Our approximate solution $u_h(x)$ can then be written in terms of its expansion coefficients and the basis functions as
\begin{equation}
\label{equ:appro}
	u_h(x) = \sum_{i = 1}^n u_i\varphi_i(x),
\end{equation} 
where we note that this particular basis has the convenient property that $u_h(x_j) = u_j$, for j = 1,...n.

A Galerkin formulation can be stated as: Find $u_h \in V_h$ such that
\begin{equation}\label{equ:formula}
	\int_0^1u_h'(x)v'(x) = \int_0^1f(x)v(x) dx, \quad \forall v \in V_h 
\end{equation}
In particular, \eqref{equ:formula} should be satisfied for $v = \varphi_i$ , $i = 1,...,n$, which leads to $n$ equations of the form
\begin{equation}
	\int_0^1 u_h'(x)\varphi_i(x)dx = \int_0^1\varphi_i(x)f(x)dx, \quad i = 1,...,n
\end{equation}
Insert the expression \eqref{equ:appro} for the approximate solution and its derivative, $u_h'(x) = \sum_{i=1}^n u_i\varphi'_i(x)$.
Change the order of interation:
\begin{equation}
	\sum_i^n u_j\left[\int_0^1\varphi_i'(x) \varphi_j'(x)\right]= \int_0^1\varphi_i(x) dx, \quad i = 1,...,n
\end{equation}
This is a linear system of equations $A\mathbf{u} = \mathbf{f}$, with $A = [a_{ij}]$, $\mathbf{u} = [u_i]$, $\mathbf{f} = [f_i]$, for $i,j = 1,...,n$, where
\begin{align}
	a_{ij} = \int_0^1\varphi'_i(x)\varphi'_j(x)dx \\
	f_i  = \int_0^1f(x)\varphi_i(x) dx
\end{align}

\subsubsection{Parallel computation strategy}
The specific equation we use to test here
is
\begin{equation}
    -u''(x) = \pi^2*\sin(\pi x)
\end{equation}
First we try Gauss Seidel method to solve the linear system defined in the formal section. The brief introduction of the Gauss Seidel method is shown belew
\begin{algorithm}
\caption{Gauss seidel method}\label{alg:gauss_seidel}
\begin{algorithmic}
\Require $A = (a_{ij})$,$f = f_i$
\State $\epsilon=1$
\While{$\epsilon \leq 1e-3$}
\State \textbf{for} $i$ \textbf{from} $1$ \textbf{until} $n$ \textbf{do}
\State \quad $\sigma \leftarrow 0$
\State \quad \textbf{for} $j$ \textbf{from} $1$ \textbf{until} $n$ \textbf{do}
\State \quad \quad \textbf{if} $j \neq i$
\textbf{then}
\State \quad  \quad  $\sigma$ $\leftarrow \sigma + a_{ij}u_j$
\State \quad  \quad \textbf{end if}
\State \quad \textbf{end} ($j$ - loop)
\State $u_i \leftarrow \frac{1}{a_{ii}} (f_i - \sigma)$
\EndWhile
\end{algorithmic}
\end{algorithm}

The numerical result is shown in the Figure \ref{fig:result comparison}. If we use serial version, it will take $3.12e-4$ for $n=32000$ and $1e-3$ for $n = 64000$ 
\begin{figure}
    \centering
    \includegraphics[width = 0.5\linewidth]{../CPP_code/1D_problem/openmp_version_gauss_seidel/result.png}
    \caption{The comparison result of the exact result and the numerical result, when the domain is decomposed into 10 equal part.}
    \label{fig:result comparison}
\end{figure}


The advantage of the Gauss Seidel method is that it's easy to parall computing. The $u_i$ can be updated independently because they only need the information of the last step. We try to use Openmp and MPI to parallel compute the result.  

For OpenMP, the $A,f$ and $u$ can be all treated as a shared variable and $u$ is updated altogether once after each iteration.
The computation time used with OpenMP to update each step compared with the thread used is shown in the Figure \ref{fig:openmp result} and Table \ref{tab:openmp table}. It's obvious that the time used is proportional decrease with the number of the thread increase.
\begin{figure}
    \centering
    \includegraphics[width = 0.5\linewidth]{../CPP_code/1D_problem/openmp_version_gauss_seidel/open_mp_result.png}
    \caption{The computation time openmp takes with respect to the number of threads}
    \label{fig:openmp result}
\end{figure}

\begin{table}[htp]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    Thread Num & 1 & 2 & 4 & 8 & 16 & 32 \\
    \hline
    n =32000     &3.06 e-04&1.66 e-04&7.82 e-05&3.56 e-05&2.05 e-05&1.66 e-05  \\
    \hline
    n =64000     &1.41 e-03&6.03 e-04&2.72 e-04&1.32 e-04&4.24 e-05&2.73 e-05\\
    \hline
    \end{tabular}
    \caption{This table records the computation time openmp cost for different number of thread to compute the different number of gird.}
    \label{tab:openmp table}
\end{table}

MPI is also tried to use to this problem, where each thread is used to calculate some row of the vector. The "ReduceAll" is used to distribute the result to each worker. The result is shown in Figure \ref{fig:mpi result}  and Tabel \ref{tab:mpi table}, but the time cannot be decreased dramatically.
This is because much time is wasted on communication. A more careful treatment to the memory distribution is needed to make it faster. However, I don't have enough time to do it now.
\begin{figure}
    \centering
    \includegraphics[width = 0.5\linewidth]{../CPP_code/1D_problem/mpi_version_gauss_seidel/mpi_result.png}
    \caption{The computation time openmp takes with respect to the number of threads}
    \label{fig:mpi result}
\end{figure}
\begin{table}[htp]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    Thread Num & 1 & 2 & 4 & 8 & 16 & 32 \\
    \hline
    n =32000     &3.65e-04 &2.51e-04 &2.18e-04 &2.38e-04 &3.24e-04 &2.85e-04  \\
    \hline
    n =64000     &8.05e-04 &5.10e-04 &4.35e-04 &4.36e-04 &5.01e-04 &7.02e-04\\
    \hline
    \end{tabular}
    \caption{This table records the computation time mpi cost for different number of thread to compute the different number of gird.}
    \label{tab:mpi table}
\end{table}

We also tried a CUDA code to make it become faster. The strategy is that we want each thread deal with one row individually. The matrix $A$ and $f$ is used repeatedly on GPU, so there are little communicate time of the CUDA code, the time of each iteration is about $1e-4$.


\subsection{Two Dimensional case}

In two dimensional case, we tried the case
\begin{equation}
    -\nabla u = 1 
\end{equation}
The idea of solving this equation can be stated briefly here.
Similar as the one dimensional case, we need divide the 2 dimensional domain into different part. There are several code can do this like \cite{persson2004simple} with a C++ version in https://github.com/pgebhardt/libdistmesh. However, it seems to dependent on a geometry library called Qhull. Therefore I just used a Julia code to generate the mesh and then read it in the C++. The generated mesh on different irregular domain can be seen in Figure \ref{fig:irregular mesh}.
\begin{figure}
    \centering
    \includegraphics[width = 0.4\linewidth]{../CPP_code/2D_problem/julia_utilities/square.png}
    \includegraphics[width = 0.4\linewidth]{../CPP_code/2D_problem/julia_utilities/sphere.png}
    \includegraphics[width = 0.4\linewidth]{../CPP_code/2D_problem/julia_utilities/five_line.png}
    \includegraphics[width = 0.4\linewidth]{../CPP_code/2D_problem/julia_utilities/half_sphere.png}
    \caption{The mesh generated on  different domain.}
    \label{fig:irregular mesh}
\end{figure}

Then we can define the basis function on different point in the domain, with a linear function equal 1 on itself and 0 on other point in the domain. To be specific, we define the triangulation of the domian $\Omega$ into triangular elements $T^k$, $k= 1,\cdots,K$ and nodes $\mathbf{x}_i,i = 1,\cdots ,n$. Consider the space $V_h$ of continuous functions that are linear within each element.
Use a nodal basis $V_h = span\{\varphi_1,\cdots,\varphi_n\}$ defined by
\begin{equation}
    \varphi_i\in V_h, \quad \varphi_i(\mathbf{x}_j) = \delta_{ij}, \quad 1\leq i,j\leq n
\end{equation}

A function $v\in V_h$ can then be written as
\begin{equation}
    v =\sum_{i=1}^n v_i\varphi_i(\mathbf{x})
\end{equation}
with the nodal interpretation $v(\mathbf{x}_j) = v_j$.
\subsubsection{Local Basis Functions}
Consider a triangular element $T^k$ with local nodes $\mathbf{x}_1^k,\mathbf{x}_2^k,\mathbf{x}_3^k$. The local basis functions $\mathcal{H}_1^k,\mathcal{H}_2^k,\mathcal{H}_3^k$ are linear functions:
\begin{equation}
    \mathcal{H}_\alpha^k = c_\alpha^k + c_{\alpha,x}^kx + c_{\alpha,y}^ky 
\end{equation}
with the property that $\mathcal{H}_\alpha^k(\mathbf{x}_\beta) = \delta_{\alpha\beta}$.
The elementary matrix for an element $T^k$ becomes 
\begin{equation}
    A^k_{\alpha\beta} = \int_{T^k} \frac{\partial \mathcal H^k_\alpha}{\partial x}\frac{\partial \mathcal H^k_\beta}{\partial x} + \frac{\partial  \mathcal H^k_\alpha}{\partial y}\frac{\partial \mathcal H^k_\beta}{\partial y} dx = Area^k( c_{\alpha,x}^kc_{\beta,x}^k + c_{\alpha,y}^kc_{\beta,y}^k)
\end{equation}
The elementary load becomes
\begin{equation}
b_{\alpha}^k = \int_{T^k} f \mathcal H_\alpha^k dx
= \frac{Area^k}{3} f
\end{equation}
Therefore the finite element convert the PDE into a Linear system
\begin{equation}
    A u = b
\end{equation}
\subsection{parallel computation result}
First we present some result we get from the serial version of code in Figure \ref{fig:2D_result}, with time about $1e-5$ seconds for each iteration when we divide the sphere into 72 triangular.
\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{../CPP_code/2D_problem/cpp_code/serial_version/square/result.png}
    \includegraphics[width=0.3\linewidth]{../CPP_code/2D_problem/cpp_code/serial_version/sphere/result.png}
    \includegraphics[width=0.3\linewidth]{../CPP_code/2D_problem/cpp_code/serial_version/test3/result.png}
    \caption{The result of the two dimensional Finite element method}
    \label{fig:2D_result}
\end{figure}

The first strategy  is to use OpenMP to solve the problem, the same as the former 1D process, each process can update some point in each iteration. The computation time for each iteration is shown in the Figure \ref{fig:2D openMP time} with  16 process the time used is about $2.93e-5$ seconds which is important decrease with respect to the serial version. However, with the increase of the process, the time used cannot be reduced anymore. I think this may because of the communication time.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{CPP_code/2D_problem/cpp_code/openmp/sphere/time_OpenMP_2D.png}
    \caption{The time used with respect to the number of the process in OpenMP}
    \label{fig:2D openMP time}
\end{figure}

Another way to reduce the computation time is use the Sparse Matrix. I use the julia to implement the Sparse Matrix code. The mass matirx $A$ in the finite element code is very sparse. I try to record them in a Compressed sparse column version with three vector (val, row\_ind, col\_ptr), where val is an array of the (top-to-bottom, then left-to-right) non-zero values of the matrix; row\_ind is the row indices corresponding to the values; and, col\_ptr is the list of val indexes where each column starts. Then I use the MPI package of the julia to do the parallel computation \cite{Byrne2021}. The computation time compared with the number of thread can be seen in figure

\begin{figure}
    \centering
    \includegraphics{julia_code/2D/MPI/sphere/julia_MPI_2D.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\section{Discussion}
\printbibliography %Prints bibliography
\end{document}
